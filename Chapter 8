
1. 降维(Dimensionality Reduction)
	A. 通常情况下，降维会加速训练过程，但会使训练过程更加复杂和难以维护，同时也会降低一些识别效果。
	B. 有些情况下，降维反而会提高识别效果，因为去除了噪声和不必要的特征等等。
	C. 降维还可用于数据可视化，通过减少数据的维度，可以将髙维数据用图形表示出来。
	D. 髙维数据存在稀疏化的风险，在二维空间中，两个随机点的平均距离(欧式距离)为0.52，在三维空间中，平均距离为0.66，在一百万维空间中，平均距离
	   为408.25。数据维度越高就越可能稀疏，从而过拟合的风险就越高。
	E. 理论上，解决髙维数据过拟合风险可以增加训练集，但是实际上，随着维度的增加，为了使训练集保持稠密，所需的数据是呈指数增长的。
	F. projection(投影)
		A) 将髙维数据投影到低维空间中，来达到降维的目的。
		B) 投影在很多情况下并不是最好的降维方式，比如数据发生卷曲时，投影会将数据压扁，但是我们真正想要的是展开。
	G. Manifold Learning(复写学习)
		A) 一个d维的manifold表示在n(n > d)维空间中类似一个d维超平面的局部区域。
		B) manifold learning基于manifold assumption：现实世界中大多数髙维数据都与一个更低维的maniflod相近。
		C) manifold assumption隐含了另一假设：使用降维能够使问题变得简单。但是，这个假设有时候并不成立。所以当使用降维技术对数据进行降
		   维时，显然会加快训练过程，但是最后的结果好坏与数据有关，可能好，可能坏。
	H. PCA(principal component analysis，主成分分析)
		A) 目前最流行的降维方法，它首先定义一个最接近数据的超平面，然后将髙维数据投影到该平面上。
		B) 保留方差(preserving the Variance)：为了尽可能的防止信息丢失，需要尽量保存各个轴上的数据散布情况(方差，variance)。
		C) 主成分(principal components)：为了将数据降到d维，需要确定d维超平面。这个超平面由d个主成分确定，每个主成分为单位向量，它们之
		   间相互正交(一组d个正交单位向量，支撑着d维空间)，可对训练数据进行奇异值分解(singular value decomposition, SVD)得出。
		D) PCA假设数据集已经中心化了，所以在自己实现时先要进行中心化处理。
