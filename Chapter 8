
1. 降维(Dimensionality Reduction)
	A. 通常情况下，降维会加速训练过程，但会使训练过程更加复杂和难以维护，同时也会降低一些识别效果。
	B. 有些情况下，降维反而会提高识别效果，因为去除了噪声和不必要的特征等等。
	C. 降维还可用于数据可视化，通过减少数据的维度，可以将髙维数据用图形表示出来。
	D. 髙维数据存在稀疏化的风险，在二维空间中，两个随机点的平均距离(欧式距离)为0.52，在三维空间中，平均距离为0.66，在一百万维空间中，平均距离
	   为408.25。数据维度越高就越可能稀疏，从而过拟合的风险就越高。
	E. 理论上，解决髙维数据过拟合风险可以增加训练集，但是实际上，随着维度的增加，为了使训练集保持稠密，所需的数据是呈指数增长的。
	F. projection(投影)
		A) 将髙维数据投影到低维空间中，来达到降维的目的。
		B) 投影在很多情况下并不是最好的降维方式，比如数据发生卷曲时，投影会将数据压扁，但是我们真正想要的是展开。
	G. Manifold Learning(复写学习)
		A) 一个d维的manifold表示在n(n > d)维空间中类似一个d维超平面的局部区域。
		B) manifold learning基于manifold assumption：现实世界中大多数髙维数据都与一个更低维的maniflod相近。
		C) manifold assumption隐含了另一假设：使用降维能够使问题变得简单。但是，这个假设有时候并不成立。所以当使用降维技术对数据进行降
		   维时，显然会加快训练过程，但是最后的结果好坏与数据有关，可能好，可能坏。
	H. PCA(principal component analysis，主成分分析)
		A) 目前最流行的降维方法，它首先定义一个最接近数据的超平面，然后将髙维数据投影到该平面上。
		B) 保留方差(preserving the Variance)：为了尽可能的防止信息丢失，需要尽量保存各个轴上的数据散布情况(方差，variance)。
		C) 主成分(principal components)：为了将数据降到d维，需要确定d维超平面。这个超平面由d个主成分确定，每个主成分为单位向量，它们之
		   间相互正交(一组d个正交单位向量，支撑着d维空间)，可对训练数据进行奇异值分解(singular value decomposition, SVD)得出。
		D) PCA假设数据集已经中心化了，所以在自己实现时先要进行中心化处理。
		E) 投影：设训练集为X，主成分向量组成的矩阵为Wd，则投影即是计算X_proj = X * Wd。重构：X = X_proj * Wd.T。重构后的数据与原来的
		   数据有差别，因为投影时丢失了一些信息，但是重要信息没丢。这个过程相当于对数据进行压缩，保留有用信息。
		F) explained variance ratio：表达了沿着每个主成分的轴的数据散布情况的分数，和为1。分数越大表示携带信息越多，反之越少。
		G) 除了进行数据可视化设置维度为2或3之外，其他情况下应该指定explained variance ratio，而不是直接指定要降到的维度，这样保证信息
		   不会过于丢失(如设置explained variance ratio为0.95)。另一个选择是画出随维度(最终要降到的维度)的变化，explained variance
		   ratio的变化趋势，然后选择没有显著增长的一个点。
		H) 增量PCA(Incremental PCA，IPCA)：PCA需要所有训练集都在内存中，不适应超大数据集。IPCA类似与MGD，将训练集划分成多个
		   mini-batch，然后一个batch一个batch进行奇异值分解。这保证了算法的适应性，不仅不需要大内存，同时也可进行线上训练。
