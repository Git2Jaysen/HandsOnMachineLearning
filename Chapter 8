
1. 降维(Dimensionality Reduction)
	A. 通常情况下，降维会加速训练过程，但会使训练过程更加复杂和难以维护，同时也会降低一些识别效果。
	B. 有些情况下，降维反而会提高识别效果，因为去除了噪声和不必要的特征等等。
	C. 降维还可用于数据可视化，通过减少数据的维度，可以将髙维数据用图形表示出来。
	D. 髙维数据存在稀疏化的风险，在二维空间中，两个随机点的平均距离(欧式距离)为0.52，在三维空间中，平均距离为0.66，在一百万维空间中，平均距离
	   为408.25。数据维度越高就越可能稀疏，从而过拟合的风险就越高。
	E. 理论上，解决髙维数据过拟合风险可以增加训练集，但是实际上，随着维度的增加，为了使训练集保持稠密，所需的数据是呈指数增长的。
	F. projection(投影)
		A) 将髙维数据投影到低维空间中，来达到降维的目的。
		B) 投影在很多情况下并不是最好的降维方式，比如数据发生卷曲时，投影会将数据压扁，但是我们真正想要的是展开。
	G. Manifold Learning(复写学习)
		A) 一个d维的manifold表示在n(n > d)维空间中类似一个d维超平面的局部区域。
		B) manifold learning基于manifold assumption：现实世界中大多数髙维数据都与一个更低维的maniflod相近。
		C) manifold assumption隐含了另一假设：使用降维能够使问题变得简单。但是，这个假设有时候并不成立。所以当使用降维技术对数据进行降
		   维时，显然会加快训练过程，但是最后的结果好坏与数据有关，可能好，可能坏。
	H. PCA(principal component analysis，主成分分析)
		A) 目前最流行的降维方法，它首先定义一个最接近数据的超平面，然后将髙维数据投影到该平面上。
		B) 保留方差(preserving the Variance)：为了尽可能的防止信息丢失，需要尽量保存各个轴上的数据散布情况(方差，variance)。
		C) 主成分(principal components)：为了将数据降到d维，需要确定d维超平面。这个超平面由d个主成分确定，每个主成分为单位向量，它们之
		   间相互正交(一组d个正交单位向量，支撑着d维空间)，可对训练数据进行奇异值分解(singular value decomposition, SVD)得出。
		D) PCA假设数据集已经中心化了，所以在自己实现时先要进行中心化处理。
		E) 投影：设训练集为X，主成分向量组成的矩阵为Wd，则投影即是计算X_proj = X * Wd。重构：X = X_proj * Wd.T。重构后的数据与原来的
		   数据有差别，因为投影时丢失了一些信息，但是重要信息没丢。这个过程相当于对数据进行压缩，保留有用信息。
		F) explained variance ratio：表达了沿着每个主成分的轴的数据散布情况的分数，和为1。分数越大表示携带信息越多，反之越少。
		G) 除了进行数据可视化设置维度为2或3之外，其他情况下应该指定explained variance ratio，而不是直接指定要降到的维度，这样保证信息
		   不会过于丢失(如设置explained variance ratio为0.95)。另一个选择是画出随维度(最终要降到的维度)的变化，explained variance
		   ratio的变化趋势，然后选择没有显著增长的一个点。
		H) 增量PCA(Incremental PCA，IPCA)：PCA需要所有训练集都在内存中，不适应超大数据集。IPCA类似与MGD，将训练集划分成多个
		   mini-batch，然后一个batch一个batch进行奇异值分解。这保证了算法的适应性，不仅不需要大内存，同时也可进行线上训练。
		I) 随机PCA(Randomized PCA, RPCA)：使用随机算法来求解主成分的估计量，比IPCA更快。
		J) 核PCA(kernel PCA，kPCA)：与SVM一样，可以使用kernel trick来进行主成分分析，擅长于保留投影后实例组成的簇，有时候甚至能展开
		   卷曲的数据集。
			a. 因为kPCA是无监督学习，所以没有准确的度量方式来让我们选取具体的核，但是因为PCA降维一般都是有监督学习的预处理部分，所
			   以可以利用网格搜索来选择核和超参数，使得预测器在任务上的性能最好。
			b. 另一种方法是选择重构后产生的重构错误最小的核和超参数。但是，使用了核的重构不像线性PCA那样容易。解决方法是构建
			   pre-image，即训练一个回归模型，把投影后的实例当作训练集，原本的训练集当作目标来训练，然后对投影后的实例进行预测，
			   得到pre-image。最后重构误差就是pre-image和原本训练集的误差。
		K) 局部线性嵌入(Locally Linear Embedding，LLE)：非线性降维技术，属于manifold learning，但不使用投影。
			a. 简要的步骤如下：
				a) 计算每个实例与其最近邻居的线性相关性。
				b) 寻找一个最能反映上述相关性的低维表示。
			b. 特别适合展开卷曲的数据，特别是噪声比较少时，但是对大数据集适应性不好(时间复杂度高)。
		M) 其他降维技术
			a. MDS(Multidimensional Scaling)：保留实例之间的距离关系。
			b. Isomap：首先通过连接实例和它最近的邻居来构建图，然后保留图上的距离来降维。
			c. t-SNE(t-Distributed Stochastic Neighbor Embedding)：通过保留聚集相似的实例、分开不相似的实例来降维。最常用于
			   数据可视化，特别是高维空间中实例组成的簇。
			d. LDA(Linear Discriminant Analysis，线性判别分析)：实际上是一种分类算法，但是在训练的时候，它会学习最有区分力的轴
			   和类别，这些轴可以用来定义投影的超平面，投影后会使各个类别尽可能的分开。
		
		
