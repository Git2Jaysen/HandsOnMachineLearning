
1. 线性回归(以RMSE或MSE为损失函数)，求解最优参数的4种方法：
	A. Normal Equation
		a. 优点：简单直接，可以找到最优参数. 对特征数量较少，内存可以直接装下的数据适应性良好。
		b. 缺点：不适应特征数量庞大以及内存装不下的数据集。
	B. Gradient Descent
		A) 最终结果与学习率有很大关系。如果学习率太小，那么梯度下降算法需要很多次迭代才能收敛，非常耗时。如果学习率太大，那么梯度下降算法可能会发散，导
		   致不能收敛，找不到好的参数值。
		B) 并不是所有的损失函数都会像MSE这样的凸函数是规律的碗型，可能是平原型，多山峰型。这些不规则的形状会给梯度下降算法带来两个问题：局部最小和平原
		   化。
			a. 局部最小：算法最终收敛时，求得的参数是局部最优值，而不是全局最优值。
			b. 平原化：算法在进行优化时达到了很长一段平原，导致需要非常长的时间才能收敛，且最终收敛时可能达不到全局最优值。
		C) 使用梯度下降算法时，应该保证保证所有的特征都处在相似的范围内(使用StandardScaler, MinMaxScaler等)，这样才不会使算法陷入平原化，导致训练
		   时长增加。
		D) Batch Gradient Descent
			a. 当特征数量非常庞大时，比Normal Equation更快，效率更高，而且能达到全局最优。
			b. 为了寻找最佳的学习率，可以使用网格搜索，但是如何设置合理的迭代次数，才能使每次所搜耗时不会太长，而是恰好合适？
				a) 设定一个误差界tolerance，如果前后两次迭代的误差小于tolerance，则停止。
				b) tolerance = O(1 / iterations)，如果tolerance缩小1/10，那么iterations就会增加10倍。
			c. 每一步都使用全部的训练数据来计算参数，当训练集很大时，训练过程会非常耗时。
		E) Stochastic Gradient Descent
			a. 优点：每一次只取一个样本来计算参数，解决了Batch Gradient Descent训练耗时的问题，能够处理超大型数据集，且有利于跳出局部最小。
			b. 缺点：因为是随机选择样本，导致最终算法会在局部最小处来回波动，最终的结果虽然很好，但不是最优解，而是接近最优解的一个解。
			c. 在最优解附近来回波动的解决方案：类似于模拟退化，先设定一个较大的学习率以跳出局部最优，然后逐渐减小学习率，使算法收敛，接近全局最优。
		F) Mini-Batch Gradient Descent
			a. Batch Gradient Descent和Stochastic Gradient Descent的折衷，每次在求解参数时，既不利用全部数据，也不只利用一个数据，而是利用小批
			   量的数据。与Stochastic Gradient Descent相比，Mini-Batch Gradient Descent能够利用GPU等硬件来获得效率上的提升(矩阵运算)，但是使
			   得更难跳出局部最优。
			b. 最终的结果与Stochastic Gradient Descent一样，是接近最优解的一个解。

总结
Algorithm		Large m		Whole Set 		Large n		Hyperparams		Scaling required	Scikit-Learn
Normal Equation		Fast		Yes			Slow		0			No			LinearRegressor
Batch GD		Slow		Yes			Fast		2			Yes			No
Stochastic GD		Fast		No			Fast		>=2			Yes			SGDRegressor
Mini-batch GD		Fast		No			Fast		>=2			Yes			No

2. 多项式回归
	A. 线型回归无法处理非线性数据，这时候可以对数据进行多项式拟合。
	B. 多项式回归的想法很简单，即将数据处理成多项式特征，然后做线型回归，最终的结果与多项式回归等价。Scikit-Learn中的PolynomialFeatures使用
	   的正是这种想法。不仅如此，多项式回归还能寻找特征之间的关系，这是线性回归做不到的，因为在生成多项式特征时，会产生形如a×b、a**2 * b这样的
	   形式。
	C. 如何设置恰当的最高项次数，使得既不过拟合也不欠拟合？
		A) 交叉验证。Scikit-Learn.model_selection.cross_val_score。
		B) 学习曲线。Scikit-Learn.model_selection.learning_curves。
			a. 每次选取部分训练集数据训练模型，并得出训练数据和验证数据的性能(评价指标)，然后画出随着训练数据增大，训练数据和验证数据
			   性能的曲线。
			b. 欠拟合分析：高偏差(high bias)，即最终训练集和验证集的性能相差较小，但与预期的性能值相差较大，此时增加数据量没有任何用
			   处。
			c. 过拟合分析：高方差(high variance)，即最终训练集和验证集的性能相差较大，但与预期的性能值相差较小，此时增加数据量会减小
			   训练集和验证集的差距。
			d. 恰好合适：合适的偏差和方差，即最终训练集和验证集的性能相差较小，但与预期的性能值相差较小。
			e. bias-variance tradeoff：减小偏差(bias)会增加方差(variance)，增加偏差(bias)会减小方差(variance)。
	D. 简单地减少过拟合的方法是减小最高项次数。
			
3. 正则化Regularization
	A. 减少模型过拟合，综合考虑损失函数值和模型复杂度，在保证损失函数值尽可能小的同时，减小模型复杂度。
	B. L2正则化：Ridge Regularization，可以使用Normal Equation和Gradient Descent求解。
	C. L1正则化：Lasso Regularization，会自动进行特征选择，忽略不重要的特征。
	D. L1正则化和L2正则化的混合：Elastic Net。
	E. Early Stopping。
		A) 训练过程中，验证集误差首先会快速减小，然后缓慢增加，Early Stopping的做法是在验证集误差达到最小时停止训练。
		B) 当使用SGD和MGD时，验证集误差曲线并不光滑，存在小波动，这是不好确定最小值。一个做法是，当达到当前最小值时，记录该点的参数，然后
		   设置一个tolerance period(再继续迭代多少次)，如果在这个period中，没有另外更小的值，则停止训练，并回退到最小值点。
		C) 简单易行，通用。
	F. 在使用Ridge, Lasso, Elastic Net进行正则化之前，记得范围化数据，使数据具有相似的范围(StandardScaler, MinMaxScaler)。
	G. 什么时候选用Linear Regression, Ridge, Lasso 或 Elastic Net?
		A) 通常应该避免使用不加正则化的Linear Regression。
		B) Ridge正则化是默认的选择。
		C) 如果认为仅仅只有一部分特征是有用的，可以选择Lasso和Elastic Net正则化，因为它们能够进行特征选择。但是，Elastic Net比Lasso更
		   好，因为当特征数量远大于训练样本数或者一些特征有很强的相关性时，Lasso正则化表现的不是很稳定。
		   
4. 逻辑斯蒂回归(Logistic Regression)
	A. 损失函数使用的是对数损失，没有Normal Equation，但因为时凸函数，所以可以使用Gradient Descent。
	B. 逻辑斯蒂回归同样可以使用Ridge, Lasso, Elastic Net正则化。
	C. Softmax Regression(multi-class regression, multi-nomial regression)：能够解决多类别分类问题，但不需要像OVA和OVO策略一样训练多
	   个二分类器。
		A) 使用交叉熵作为损失函数。


