
1. 集成学习(Ensemble Learning)
	A. 群体智慧(wise of crowd)：聚集多个预测器(predictor)的预测结果做出的预测往往比单个预测器的预测结果要好很多。
	B. 在机器学习项目中，集成学习经常被用到。如果已经训练好了一些不错的预测器，通过集成学习能够获得结果的提升。
	C. 集成学习在预测器之间尽可能相互独立时效果最好，如果预测器都产生相同类型的错误，那么在预测错误时，反而会加重错误，造成准确率的下降。
	D. voting
		A) 如果有一些预测效果较好的预测器，想要提升预测效果，最简单的方法就是使用多数投票(majority voting)的方法。它通过使用不同的算法来
		   训练以产生不同类型的错误，保证最终的集成预测效果。
		B) 多数投票分为hard和soft两种。
			a. hard majority voting：直接根据最终的预测结果进行多数投票。
			b. soft majority voting：对预测的概率或分数进行某种策略进行融合(加和求平均、加权等)，然后得出预测结果。通常比hard要好。
		C) 对应Scikit-Learning中VotingClassfier。参数voting='hard'和voting='soft'选择不同的形式。
	E. Bagging and Pasting
		A) 通过利用同一预测器在不同的训练集子集上训练，并将这些预测器的结果进行融合(对于分类，可以是hard voting和soft voting;对于回归，
		   可以是取平均)，来获得最终的集成预测结果。
		B) 训练子集采样有两种方法：放回和不放回。如果放回，则称为bagging;如果不放回，则称为pasting。
		C) 因为是在训练集子集上训练，所以最终每个预测器都有很大的偏差(bias)，但是通过集成，最终得到的模型与在全部训练集上训练的单个预测器
		   相比，具有相近的偏差(bias)，但是却有更小的方差(variance)。
		D) Bagging and Pasting适应性很好，因为训练和预测可以并行进行，提高了效率。
		E) Bagging通常会得到更好的模型，但是如果有时间和计算资源的话，可以使用交叉验证来确定哪一个更好。
		F) Out-of-Bag(oob)：在对训练集进行采样时，只有一部分训练数据被用于训练，其他的数据对当前某个预测器来说是不可见的，可以用来做验证。
		   在Scikit-Learn的BaggingClassifier中设定oob_score=True即可。
		G) Random Subspaces: 对特征进行采样，而不是对样本进行采样，对输入维数很高时非常有用。
		H) Random Patches：同时对样本和特征进行采样。
		I) Random Forests：利用bagging方法对决策树进行集成。在Scikit-Learn中，有专门的类RandomForestClassifier和RandomForestRegr
		   essor。
		J) 在运用决策树时，重要的特征会更加接近根结点，不重要的特征会更接近叶结点，所以在随机森林中，可以根据特征的平均深度来估计特征的重要
		   性。在Scikit-Learn中，feature_importances_表示了这种重要性。通过使用随机森林，可以对特征重要性有一个清晰的认识，方便做特征
		   选择。
	F. Boosting
		A) Boosting的思想是训练一系列的预测器，并且在训练当前预测器时矫正前一个预测器的错误。常用的Boosting方法有两种：Adaptive Boost
		   ing(又称AdaBoost)和Gradient Boost。
		B) AdaBoost
			a. 思想是尽量去拟合前一个预测器没有拟合的实例。每个实例都有一个权重，未得到正确预测的实例权重会增加，使得被下一个预测器
			   选中的可能性增加，这个过程有点类似于梯度下降，只不过不是优化参数。
			b. 当所有预测器训练完成后，预测的过程和bagging或者pasting非常类似，不过每一个预测器的结果都有一个权重。
			c. 缺陷：无法并行。
		C) Gradient Boost
			a. 与AdaBoost调整实例权重不同，Gradient Boost尝试拟合前一个预测器残余的错误(误差，error，真实值与预测值之差)。
			b. 
			
			
