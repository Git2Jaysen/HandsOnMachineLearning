
1. 决策树
	A. 决策树能够适应分类任务、回归任务，甚至是多输出任务。
	B. 决策树几乎不需要数据预处理，而且不需要做特征范围化(feature scaling)和中心化(centering)处理。
	C. 决策树做出的决定过程是清晰可知的(white box)，提供了分类规则，不像神经网络那样(black box)无法分析所利用的具体规则。
	D. Scikit-Learn中，决策树使用CART算法实现，该算法生成的决策树为二叉树，但其他算法如ID3允许有多个子结点。
	E. 对于分类问题，CART常用的划分准则有基尼指数和熵，很多情况下两者都能得到相似的结果，但基尼指数的计算复杂度更小一些，所以一般默认基尼指
	   数作为划分准则。但是，它们两者有区别，基尼指数倾向于将最高频的类单独分出一个分支，而熵倾向于生成更加平衡的树。
	F. 对于回归问题，决策树的常用划分准则为MSE。
	G. Scikit-Learn中提供了一系列的参数来正则化决策树，其中，增加min_*参数的值和减小max_*参数的值有利于防止决策树过拟合。其他正则化的方法
	   还包括预剪枝和后剪枝。
	H. 决策树缺陷：
		A) 喜欢设定正交的决策边界，使得对训练集旋转过于敏感。一种解决办法是使用PCA。
		B) 对训练集的一些细小的变化非常敏感。随机森林能解决这个问题。
